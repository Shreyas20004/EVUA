from pathlib import Path
import json
from datetime import datetime, timezone
import shutil
import ast
import traceback
from typing import Dict

from core.languages.python.stages.stage0_preprocess import preprocess_repo
from core.languages.python.stages.stage1_structural import structural_convert

PROJECT_ROOT = Path(__file__).resolve().parents[3]


def copy_stage_output(stage_dir: Path, final_dir: Path):
    """Syncs the latest stage output into final_output/ (overwrites existing files)."""
    for src_file in stage_dir.rglob("*.py"):
        rel_path = src_file.relative_to(stage_dir)
        dest_path = final_dir / rel_path
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(src_file, dest_path)


def run_pipeline(repo_path: Path, session_dir: Path, config: dict = None) -> Dict:
    """
    Run the EVUA Python migration pipeline (Stage0 â†’ Stage3).
    Generates clean intermediate folders, logs, final_output, and metadata.json.
    """
    # Setup structure
    session_dir.mkdir(parents=True, exist_ok=True)
    intermediate_dir = session_dir / "intermediate"
    logs_dir = session_dir / "logs"
    final_output = session_dir / "final_output"
    for d in (intermediate_dir, logs_dir, final_output):
        d.mkdir(parents=True, exist_ok=True)

    metadata_path = session_dir / "metadata.json"
    metadata = {
        "session_id": session_dir.name,
        "language": "python",
        "start_time": datetime.now(timezone.utc).isoformat(),
        "stages": [],
        "status": "running",
    }

    try:
        # ---------- Stage 0: Preprocessing ----------
        stage0_dir = intermediate_dir / "stage0_preprocessed"
        stage0_dir.mkdir(parents=True, exist_ok=True)
        preprocess_repo(Path(repo_path), stage0_dir)

        # Parse & basic analysis
        parse_results = []
        for py_file in stage0_dir.rglob("*.py"):
            try:
                code = py_file.read_text(encoding="utf-8")
                tree = ast.parse(code, filename=str(py_file))
                parse_results.append({
                    "file": str(py_file.relative_to(stage0_dir)),
                    "functions": sum(isinstance(n, ast.FunctionDef) for n in ast.walk(tree)),
                    "classes": sum(isinstance(n, ast.ClassDef) for n in ast.walk(tree)),
                    "status": "parsed"
                })
            except Exception as e:
                parse_results.append({
                    "file": str(py_file.relative_to(stage0_dir)),
                    "status": "error",
                    "error": str(e)
                })

        (logs_dir / "stage0_parse.json").write_text(json.dumps(parse_results, indent=2))
        metadata["stages"].append({"stage": "stage0_preprocess", "status": "ok"})
        copy_stage_output(stage0_dir, final_output)

        # ---------- Stage 1: Structural Transform ----------
        stage1_dir = intermediate_dir / "stage1_structural"
        stage1_dir.mkdir(parents=True, exist_ok=True)
        result_stage1 = structural_convert(stage0_dir, stage1_dir)

        (logs_dir / "stage1_structural.json").write_text(json.dumps(result_stage1, indent=2))
        metadata["stages"].append({"stage": "stage1_structural", "status": "ok"})
        copy_stage_output(stage1_dir, final_output)

        # ---------- Stage 2: Semantic Analysis ----------
        stage2_dir = intermediate_dir / "stage2_semantic"
        stage2_dir.mkdir(parents=True, exist_ok=True)
        semantic_results = []

        for py_file in stage1_dir.rglob("*.py"):
            target_path = stage2_dir / py_file.relative_to(stage1_dir)
            target_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(py_file, target_path)
            semantic_results.append({
                "file": str(py_file.relative_to(stage1_dir)),
                "status": "semantic_checked"
            })

        (logs_dir / "stage2_semantic.json").write_text(json.dumps(semantic_results, indent=2))
        metadata["stages"].append({"stage": "stage2_semantic", "status": "ok"})
        copy_stage_output(stage2_dir, final_output)

        # ---------- Stage 3: Dynamic Verification (placeholder) ----------
        stage3_dir = intermediate_dir / "stage3_dynamic"
        stage3_dir.mkdir(parents=True, exist_ok=True)
        dynamic_summary = {
            "total_files": len(semantic_results),
            "verified": len(semantic_results),
            "manual_review": []
        }

        (logs_dir / "stage3_dynamic.json").write_text(json.dumps(dynamic_summary, indent=2))
        metadata["stages"].append({"stage": "stage3_dynamic", "status": "ok"})
        copy_stage_output(stage3_dir, final_output)

        # ---------- Finalization ----------
        metadata["status"] = "completed"
        metadata["final_output"] = str(final_output)

    except Exception as e:
        metadata["status"] = "failed"
        metadata["error"] = str(e)
        metadata["traceback"] = traceback.format_exc()

    metadata["end_time"] = datetime.now(timezone.utc).isoformat()
    metadata["config"] = config or {}

    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=4)

    return metadata


if __name__ == "__main__":
    sample_repo = PROJECT_ROOT / "tests" / "fixtures"
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    session = PROJECT_ROOT / "sessions" / f"session_{timestamp}"
    cfg = {"docker_image": "evua-python:latest", "timeout": 300}

    meta = run_pipeline(sample_repo, session, cfg)
    print("Pipeline completed successfully.")
    print("Final output:", session / "final_output")
    print("Logs:", session / "logs")
    print("Metadata:", session / "metadata.json")
